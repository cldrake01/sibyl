@article{Zeng_Chen_Zhang_Xu_2023, title={Are Transformers Effective for Time Series Forecasting?}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26317}, DOI={10.1609/aaai.v37i9.26317}, abstractNote={Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang}, year={2023}, month={Jun.}, pages={11121-11128} }

@misc{andrychowicz2023deep,
      title={Deep Learning for Day Forecasts from Sparse Observations},
      author={Marcin Andrychowicz and Lasse Espeholt and Di Li and Samier Merchant and Alexander Merose and Fred Zyda and Shreya Agrawal and Nal Kalchbrenner},
      year={2023},
      eprint={2306.06079},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@misc{woo2022etsformer,
      title={ETSformer: Exponential Smoothing Transformers for Time-series Forecasting},
      author={Gerald Woo and Chenghao Liu and Doyen Sahoo and Akshat Kumar and Steven Hoi},
      year={2022},
      eprint={2202.01381},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2022fedformer,
      title={FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting},
      author={Tian Zhou and Ziqing Ma and Qingsong Wen and Xue Wang and Liang Sun and Rong Jin},
      year={2022},
      eprint={2201.12740},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2021informer,
      title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
      author={Haoyi Zhou and Shanghang Zhang and Jieqi Peng and Shuai Zhang and Jianxin Li and Hui Xiong and Wancai Zhang},
      year={2021},
      eprint={2012.07436},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wu2022autoformer,
      title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},
      author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
      year={2022},
      eprint={2106.13008},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wen2023transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  booktitle={International Joint Conference on Artificial Intelligence(IJCAI)},
  year={2023}
}

@misc{cryptoeprint:2021/1091,
      author = {Maikel Kerkhof and Lichao Wu and Guilherme Perin and Stjepan Picek},
      title = {No (Good) Loss no Gain: Systematic Evaluation of Loss functions in Deep Learning-based Side-channel Analysis},
      howpublished = {Cryptology ePrint Archive, Paper 2021/1091},
      year = {2021},
      note = {\url{https://eprint.iacr.org/2021/1091}},
      url = {https://eprint.iacr.org/2021/1091}
}

@misc{challu2022nhits,
      title={N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting},
      author={Cristian Challu and Kin G. Olivares and Boris N. Oreshkin and Federico Garza and Max Mergenthaler-Canseco and Artur Dubrawski},
      year={2022},
      eprint={2201.12886},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2023ring,
      title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
      author={Hao Liu and Matei Zaharia and Pieter Abbeel},
      year={2023},
      eprint={2310.01889},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{brandon2023striped,
      title={Striped Attention: Faster Ring Attention for Causal Transformers},
      author={William Brandon and Aniruddha Nrusimha and Kevin Qian and Zachary Ankner and Tian Jin and Zhiye Song and Jonathan Ragan-Kelley},
      year={2023},
      eprint={2311.09431},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lee2024tildeq,
      title={TILDE-Q: A Transformation Invariant Loss Function for Time-Series Forecasting},
      author={Hyunwook Lee and Chunggi Lee and Hongkyu Lim and Sungahn Ko},
      year={2024},
      eprint={2210.15050},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
zhang2023crossformer,
title={Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting},
author={Yunhao Zhang and Junchi Yan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=vSVLM2j9eie}
}

@misc{xu2024fits,
      title={FITS: Modeling Time Series with $10k$ Parameters},
      author={Zhijian Xu and Ailing Zeng and Qiang Xu},
      year={2024},
      eprint={2307.03756},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{woo2023learning,
      title={Learning Deep Time-index Models for Time Series Forecasting},
      author={Gerald Woo and Chenghao Liu and Doyen Sahoo and Akshat Kumar and Steven Hoi},
      year={2023},
      eprint={2207.06046},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{Barron_2019_CVPR,
author = {Barron, Jonathan T.},
title = {A General and Adaptive Robust Loss Function},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{doi:10.1177/1847979018808673,
author = {Jamal Fattah and Latifa Ezzine and Zineb Aman and Haj El Moussami and Abdeslam Lachhab},
title ={Forecasting of demand using ARIMA model},

journal = {International Journal of Engineering Business Management},
volume = {10},
number = {},
pages = {1847979018808673},
year = {2018},
doi = {10.1177/1847979018808673},

URL = {

        https://doi.org/10.1177/1847979018808673



},
eprint = {

        https://doi.org/10.1177/1847979018808673



}
,
    abstract = { The work presented in this article constitutes a contribution to modeling and forecasting the demand in a food company, by using time series approach. Our work demonstrates how the historical demand data could be utilized to forecast future demand and how these forecasts affect the supply chain. The historical demand information was used to develop several autoregressive integrated moving average (ARIMA) models by using Boxâ€“Jenkins time series procedure and the adequate model was selected according to four performance criteria: Akaike criterion, Schwarz Bayesian criterion, maximum likelihood, and standard error. The selected model corresponded to the ARIMA (1, 0, 1) and it was validated by another historical demand information under the same conditions. The results obtained prove that the model could be utilized to model and forecast the future demand in this food manufacturing. These results will provide to managers of this manufacturing reliable guidelines in making decisions. }
}

@article{HO1998213,
title = {The use of ARIMA models for reliability forecasting and analysis},
journal = {Computers & Industrial Engineering},
volume = {35},
number = {1},
pages = {213-216},
year = {1998},
issn = {0360-8352},
doi = {https://doi.org/10.1016/S0360-8352(98)00066-7},
url = {https://www.sciencedirect.com/science/article/pii/S0360835298000667},
author = {S.L. Ho and M. Xie},
keywords = {Repairable system, time series, ARIMA models, Duane model, MAD, forecasting},
abstract = {This paper investigates the approach to repairable system reliability forecasting based on the Autoregressive Integrated Moving Average (ARIMA) models. This time series technique makes very few assumptions and is very flexible. It is theoretically and statistically sound in its foundation and no a priori postulation of models is required when analysing failure data. An illustrative example on a mechanical system failures is presented. Comparison is also made with the traditional Duane model. It is concluded that ARIMA model is a viable alternative that gives satisfactory results in terms of its predictive performance.}
}

@INPROCEEDINGS{10146913,
  author={Guo, Tingyu and Tian, Boping},
  booktitle={2022 International Conference on Information Science and Communications Technologies (ICISCT)},
  title={The Study of Option Pricing Problems based on Transformer Model},
  year={2022},
  volume={},
  number={},
  pages={1-5},
  keywords={Deep learning;Analytical models;Time series analysis;Pricing;Predictive models;Transformers;Mathematical models;deep learning;option pricing;Transformer},
  doi={10.1109/ICISCT55600.2022.10146913}}

@INPROCEEDINGS{9005997,
  author={Siami-Namini, Sima and Tavakoli, Neda and Namin, Akbar Siami},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)},
  title={The Performance of LSTM and BiLSTM in Forecasting Time Series},
  year={2019},
  volume={},
  number={},
  pages={3285-3292},
  keywords={Biological system modeling;Training;Data models;Logic gates;Time series analysis;Predictive models;Recurrent neural networks},
  doi={10.1109/BigData47090.2019.9005997}}

@article{LINDEMANN2021650,
title = {A survey on long short-term memory networks for time series prediction},
journal = {Procedia CIRP},
volume = {99},
pages = {650-655},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.088},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003796},
author = {Benjamin Lindemann and Timo MÃ¼ller and Hannes Vietz and Nasser Jazdi and Michael Weyrich},
keywords = {Recurrent Neural Networks, Long short-term memory, Autoencoder, Sequence-to-Sequence Networks, Time Series Prediction},
abstract = {Recurrent neural networks and exceedingly Long short-term memory (LSTM) have been investigated intensively in recent years due to their ability to model and predict nonlinear time-variant system dynamics. The present paper delivers a comprehensive overview of existing LSTM cell derivatives and network architectures for time series prediction. A categorization in LSTM with optimized cell state representations and LSTM with interacting cell states is proposed. The investigated approaches are evaluated against defined requirements being relevant for an accurate time series prediction. These include short-term and long-term memory behavior, the ability for multimodal and multi-step ahead predictions and the according error propagation. Sequence-to-sequence networks with partially conditioning outperform the other approaches, such as bidirectional or associative networks, and are best suited to fulfill the requirements.}
}

@article{KAREVAN20201,
title = {Transductive LSTM for time-series prediction: An application to weather forecasting},
journal = {Neural Networks},
volume = {125},
pages = {1-9},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020300010},
author = {Zahra Karevan and Johan A.K. Suykens},
keywords = {Transductive learning, Long short-term memory, Weather forecasting},
abstract = {Long Short-Term Memory (LSTM) has shown significant performance on many real-world applications due to its ability to capture long-term dependencies. In this paper, we utilize LSTM to obtain a data-driven forecasting model for an application of weather forecasting. Moreover, we propose Transductive LSTM (T-LSTM) which exploits the local information in time-series prediction. In transductive learning, the samples in the test point vicinity are considered to have higher impact on fitting the model. In this study, a quadratic cost function is considered for the regression problem. Localizing the objective function is done by considering a weighted quadratic cost function at which point the samples in the neighborhood of the test point have larger weights. We investigate two weighting schemes based on the cosine similarity between the training samples and the test point. In order to assess the performance of the proposed method in different weather conditions, the experiments are conducted on two different time periods of a year. The results show that T-LSTM results in better performance in the prediction task.}
}

@INPROCEEDINGS{8614252,
  author={Siami-Namini, Sima and Tavakoli, Neda and Siami Namin, Akbar},
  booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  title={A Comparison of ARIMA and LSTM in Forecasting Time Series},
  year={2018},
  volume={},
  number={},
  pages={1394-1401},
  keywords={Time series analysis;Forecasting;Predictive models;Autoregressive processes;Economics;Deep learning;Data models;Deep Learning, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Forecasting, Time Series Data},
  doi={10.1109/ICMLA.2018.00227}}

@article{CAO2019127,
title = {Financial time series forecasting model based on CEEMDAN and LSTM},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {519},
pages = {127-139},
year = {2019},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.11.061},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118314985},
author = {Jian Cao and Zhi Li and Jian Li},
keywords = {Financial time series forecasting, EMD-LSTM prediction, CEEMDAN-LSTM prediction},
abstract = {In order to improve the accuracy of the stock market prices forecasting, two hybrid forecasting models are proposed in this paper which combine the two kinds of empirical mode decomposition (EMD) with the long short-term memory (LSTM). The financial time series is a kind of non-linear and non-stationary random signal, which can be decomposed into several intrinsic mode functions of different time scales by the original EMD and the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). To ensure the effect of historical data onto the prediction result, the LSTM prediction models are established for all each characteristic series from EMD and CEEMDAN deposition. The final prediction results are obtained by reconstructing each prediction series. The forecasting performance of the proposed models is verified by linear regression analysis of the major global stock market indices. Compared with single LSTM model, support vector machine (SVM), multi-layer perceptron (MLP) and other hybrid models, the experimental results show that the proposed models display a better performance in one-step-ahead forecasting of financial time series.}
}

@article{SAGHEER2019203,
title = {Time series forecasting of petroleum production using deep LSTM recurrent networks},
journal = {Neurocomputing},
volume = {323},
pages = {203-213},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.09.082},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218311639},
author = {Alaa Sagheer and Mostafa Kotb},
keywords = {Time series forecasting, Deep neural networks, Recurrent neural networks, Long-short term memory, Petroleum production forecasting},
abstract = {Time series forecasting (TSF) is the task of predicting future values of a given sequence using historical data. Recently, this task has attracted the attention of researchers in the area of machine learning to address the limitations of traditional forecasting methods, which are time-consuming and full of complexity. With the increasing availability of extensive amounts of historical data along with the need of performing accurate production forecasting, particularly a powerful forecasting technique infers the stochastic dependency between past and future values is highly needed. In this paper, we propose a deep learning approach capable to address the limitations of traditional forecasting approaches and show accurate predictions. The proposed approach is a deep long-short term memory (DLSTM) architecture, as an extension of the traditional recurrent neural network. Genetic algorithm is applied in order to optimally configure DLSTMâ€™s optimum architecture. For evaluation purpose, two case studies from the petroleum industry domain are carried out using the production data of two actual oilfields. Toward a fair evaluation, the performance of the proposed approach is compared with several standard methods, either statistical or soft computing. Using different measurement criteria, the empirical results show that the proposed DLSTM model outperforms other standard approaches.}
}

@article{DBLP:journals/corr/abs-1905-10437,
  author       = {Boris N. Oreshkin and
                  Dmitri Carpov and
                  Nicolas Chapados and
                  Yoshua Bengio},
  title        = {{N-BEATS:} Neural basis expansion analysis for interpretable time
                  series forecasting},
  journal      = {CoRR},
  volume       = {abs/1905.10437},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.10437},
  eprinttype    = {arXiv},
  eprint       = {1905.10437},
  timestamp    = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-10437.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
